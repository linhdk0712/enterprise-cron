# ğŸ”„ Job Execution Flow - Chi tiáº¿t Ä‘áº§y Ä‘á»§

## ğŸ“Š Tá»•ng quan Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduler  â”‚â”€â”€â”€â”€â–¶â”‚    NATS     â”‚â”€â”€â”€â”€â–¶â”‚   Worker    â”‚
â”‚   Service   â”‚     â”‚ JetStream   â”‚     â”‚   Service   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
       â”‚                                        â”‚
       â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    MinIO    â”‚
â”‚  Database   â”‚                        â”‚   Storage   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                                        â–²
       â”‚                                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚     API     â”‚
                  â”‚   Service   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¬ Flow 1: Job Creation (Táº¡o Job)

### Step 1: User táº¡o job qua API/Dashboard

**API Endpoint**: `POST /api/jobs`

**Handler**: `api/src/handlers/jobs.rs::create_job()`

```rust
pub async fn create_job(
    State(state): State<AppState>,
    Json(request): Json<CreateJobRequest>,
) -> Result<Json<SuccessResponse<Uuid>>, ErrorResponse>
```

**Flow**:
```
1. Validate input (name, schedule, steps, etc.)
   â”œâ”€ Check duplicate name
   â”œâ”€ Validate cron expression (if scheduled)
   â””â”€ Validate step configuration

2. Create Job struct
   â”œâ”€ Generate UUID
   â”œâ”€ Set default values (timeout, max_retries)
   â””â”€ Set trigger config (scheduled, manual, webhook)

3. Save to PostgreSQL
   â”œâ”€ JobRepository::create()
   â””â”€ INSERT INTO jobs (id, name, enabled, trigger_config, ...)

4. Save job definition to MinIO
   â”œâ”€ MinIOService::store_job_definition()
   â”œâ”€ Serialize Job to JSON
   â””â”€ Upload to: jobs/{job_id}/definition.json

5. Return job_id to user
```

**Database Changes**:
```sql
-- PostgreSQL: jobs table
INSERT INTO jobs (
    id, name, description, enabled, 
    timeout_seconds, max_retries, allow_concurrent,
    minio_definition_path, trigger_config,
    created_at, updated_at
) VALUES (
    'uuid', 'Job Name', 'Description', true,
    300, 10, false,
    'jobs/uuid/definition.json', 
    '{"scheduled": true, "manual": true, "webhook": null}',
    NOW(), NOW()
);
```

**MinIO Changes**:
```
Upload file: jobs/{job_id}/definition.json
Content: Full job definition JSON (steps, schedule, etc.)
```

---

## ğŸ”„ Flow 2: Scheduled Job Execution (Job cháº¡y theo schedule)

### Phase 1: SCHEDULER - Trigger Detection

**Binary**: `scheduler/src/main.rs`

**Main Loop**: `common/src/scheduler/engine.rs::SchedulerEngine::start()`

```rust
pub async fn start(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>>
```

**Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHEDULER POLLING LOOP (Every 10 seconds)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Poll for jobs due
   â”œâ”€ JobRepository::find_jobs_due(now)
   â”œâ”€ SELECT * FROM jobs WHERE enabled = true
   â””â”€ Filter jobs with scheduled trigger enabled

2. For each job due:
   â”œâ”€ Check trigger conditions
   â”‚  â”œâ”€ Scheduled trigger enabled?
   â”‚  â”œâ”€ Cron expression matches current time?
   â”‚  â””â”€ Next execution time <= now?
   â”‚
   â”œâ”€ Check concurrent execution
   â”‚  â”œâ”€ ExecutionRepository::has_running_execution(job_id)
   â”‚  â”œâ”€ SELECT COUNT(*) FROM job_executions 
   â”‚  â”‚  WHERE job_id = ? AND status IN ('running', 'pending')
   â”‚  â””â”€ If allow_concurrent = false AND has_running â†’ Skip
   â”‚
   â”œâ”€ Acquire distributed lock (Redis RedLock)
   â”‚  â”œâ”€ DistributedLock::acquire("schedule:job:{job_id}", 30s)
   â”‚  â”œâ”€ SET NX EX schedule:job:{job_id} {lock_value} 30
   â”‚  â””â”€ If lock failed â†’ Skip (another scheduler is processing)
   â”‚
   â”œâ”€ Create execution record
   â”‚  â”œâ”€ Generate execution_id (UUID)
   â”‚  â”œâ”€ Generate idempotency_key: "{job_id}:{uuid}"
   â”‚  â”œâ”€ ExecutionRepository::create()
   â”‚  â””â”€ INSERT INTO job_executions (
   â”‚       id, job_id, idempotency_key, status='pending',
   â”‚       trigger_source='scheduled', created_at
   â”‚     )
   â”‚
   â”œâ”€ Publish message to NATS
   â”‚  â”œâ”€ JobPublisher::publish(execution)
   â”‚  â”œâ”€ Create JobMessage { execution_id, job_id, idempotency_key }
   â”‚  â”œâ”€ Serialize to JSON
   â”‚  â”œâ”€ Publish to subject: "jobs.job_stream.{job_id}"
   â”‚  â”œâ”€ Headers: Nats-Msg-Id = idempotency_key (deduplication)
   â”‚  â””â”€ Wait for ACK from NATS
   â”‚
   â”œâ”€ Update job stats
   â”‚  â””â”€ JobRepository::update_stats(job_id, success=true)
   â”‚
   â””â”€ Release distributed lock
      â””â”€ DEL schedule:job:{job_id}
```

**Database State After Scheduler**:
```sql
-- job_executions table
id                  | job_id | status  | idempotency_key        | created_at
--------------------|--------|---------|------------------------|------------
execution-uuid-123  | job-1  | pending | job-1:unique-uuid-456  | 2025-11-26 08:00:00
```

**NATS State After Scheduler**:
```
Stream: job_stream
Subject: jobs.job_stream.{job_id}
Message: {
  "execution_id": "execution-uuid-123",
  "job_id": "job-1",
  "idempotency_key": "job-1:unique-uuid-456",
  "attempt": 1,
  "published_at": "2025-11-26T08:00:00Z"
}
Headers: {
  "Nats-Msg-Id": "job-1:unique-uuid-456"
}
```

---

### Phase 2: WORKER - Job Consumption & Execution

**Binary**: `worker/src/main.rs`

**Consumer**: `common/src/queue/consumer.rs::NatsJobConsumer::start()`

**Handler**: `common/src/worker/consumer.rs::WorkerJobConsumer::process_job()`

```rust
async fn process_job(
    job_message: JobMessage,
    job_repo: Arc<JobRepository>,
    execution_repo: Arc<ExecutionRepository>,
    // ... other dependencies
) -> Result<(), anyhow::Error>
```

**Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WORKER - MESSAGE CONSUMPTION                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Receive message from NATS
   â”œâ”€ NatsJobConsumer::start() â†’ messages.next()
   â”œâ”€ Deserialize JobMessage from JSON
   â””â”€ Log: "Processing message, stream_sequence: X"

2. Check idempotency (Exactly-once execution)
   â”œâ”€ ExecutionRepository::find_by_idempotency_key(key)
   â”œâ”€ SELECT * FROM job_executions WHERE idempotency_key = ?
   â”œâ”€ If found AND status IN (success, failed, timeout, dead_letter):
   â”‚  â”œâ”€ Log: "Job already completed, skipping"
   â”‚  â”œâ”€ ACK message
   â”‚  â””â”€ Return OK (skip processing)
   â””â”€ If found AND status IN (pending, running):
      â””â”€ Log: "Found existing execution in progress, will process it"

3. Load job metadata from PostgreSQL
   â”œâ”€ JobRepository::find_by_id(job_id)
   â”œâ”€ SELECT * FROM jobs WHERE id = ?
   â””â”€ Get: name, timeout, max_retries, minio_definition_path

4. Load full job definition from MinIO
   â”œâ”€ MinIOService::load_job_definition(job_id)
   â”œâ”€ GET jobs/{job_id}/definition.json from MinIO
   â”œâ”€ Parse JSON to Job struct
   â””â”€ Get: steps[], schedule, triggers

5. Update execution status to RUNNING
   â”œâ”€ execution.status = Running
   â”œâ”€ execution.started_at = NOW()
   â”œâ”€ ExecutionRepository::update(execution)
   â”œâ”€ UPDATE job_executions SET status='running', started_at=NOW()
   â””â”€ Publish status change event to NATS (for SSE)

6. Initialize or load Job Context
   â”œâ”€ Try load existing context from MinIO
   â”œâ”€ GET jobs/{job_id}/executions/{execution_id}/context.json
   â”œâ”€ If not found: Create new JobContext
   â””â”€ JobContext { execution_id, job_id, steps: {}, variables: {} }

7. Execute job steps sequentially
   â””â”€ For each step in job.steps:
      â”‚
      â”œâ”€ Update current_step in execution
      â”‚  â””â”€ UPDATE job_executions SET current_step = ?
      â”‚
      â”œâ”€ Check step condition (if any)
      â”‚  â””â”€ Evaluate condition expression
      â”‚
      â”œâ”€ Route to appropriate executor
      â”‚  â”œâ”€ HttpRequest â†’ HttpExecutor
      â”‚  â”œâ”€ DatabaseQuery â†’ DatabaseExecutor
      â”‚  â”œâ”€ FileProcessing â†’ FileProcessingExecutor
      â”‚  â””â”€ Sftp â†’ SftpExecutor
      â”‚
      â”œâ”€ Execute step with retry logic
      â”‚  â”œâ”€ Attempt 1: executor.execute(step, context)
      â”‚  â”œâ”€ If failed: Wait with exponential backoff
      â”‚  â”œâ”€ Attempt 2: executor.execute(step, context)
      â”‚  â””â”€ ... up to max_retries
      â”‚
      â”œâ”€ Store step output in context
      â”‚  â”œâ”€ context.set_step_output(step_id, output)
      â”‚  â””â”€ Output: { status, data, headers, timing }
      â”‚
      â”œâ”€ Persist context to MinIO after each step
      â”‚  â”œâ”€ MinIOService::store_context(context)
      â”‚  â”œâ”€ Serialize context to JSON
      â”‚  â””â”€ PUT jobs/{job_id}/executions/{execution_id}/context.json
      â”‚
      â””â”€ If step failed AND on_failure = "stop":
         â””â”€ Break loop, mark execution as failed

8. Update final execution status
   â”œâ”€ If all steps succeeded:
   â”‚  â”œâ”€ execution.status = Success
   â”‚  â”œâ”€ execution.completed_at = NOW()
   â”‚  â”œâ”€ execution.result = "Job completed successfully"
   â”‚  â””â”€ UPDATE job_executions SET status='success', completed_at=NOW()
   â”‚
   â””â”€ If any step failed:
      â”œâ”€ execution.status = Failed
      â”œâ”€ execution.completed_at = NOW()
      â”œâ”€ execution.error = error_message
      â””â”€ UPDATE job_executions SET status='failed', error=?, completed_at=NOW()

9. Save final context to MinIO
   â”œâ”€ MinIOService::store_context(context)
   â””â”€ PUT jobs/{job_id}/executions/{execution_id}/context.json

10. Publish final status change event
    â””â”€ NATS publish: status.execution.{execution_id}

11. ACK message to NATS
    â”œâ”€ message.ack()
    â””â”€ Message removed from stream (WorkQueue retention)

12. Update job stats
    â”œâ”€ JobRepository::update_stats(job_id, success)
    â””â”€ UPDATE job_stats SET total_executions++, ...
```

---

## ğŸ” Chi tiáº¿t Step Execution (HTTP Request Example)

**Executor**: `common/src/executor/http.rs::HttpExecutor::execute()`

```rust
pub async fn execute(
    &self,
    step: &JobStep,
    context: &mut JobContext,
) -> Result<StepOutput, ExecutorError>
```

**Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTTP EXECUTOR - Execute HTTP Request Step                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Extract HTTP configuration from step
   â”œâ”€ step.step_type = JobType::HttpRequest { url, method, headers, body, auth }
   â””â”€ Resolve variables in URL/headers/body using context

2. Build HTTP request
   â”œâ”€ reqwest::Client::new()
   â”œâ”€ Set method (GET/POST/PUT/DELETE)
   â”œâ”€ Set URL (with variable substitution)
   â”œâ”€ Set headers
   â”œâ”€ Set body (if POST/PUT)
   â””â”€ Set authentication (Basic/Bearer/OAuth2)

3. Execute request with timeout
   â”œâ”€ timeout(step.timeout_seconds, client.send(request))
   â””â”€ If timeout â†’ Return ExecutorError::Timeout

4. Process response
   â”œâ”€ Get status code
   â”œâ”€ Get headers
   â”œâ”€ Read body as text/json
   â””â”€ Calculate duration

5. Create StepOutput
   â””â”€ StepOutput {
        status: "success",
        data: response_body,
        metadata: {
          "status_code": 200,
          "headers": {...},
          "duration_ms": 123
        }
      }

6. Return output (will be stored in context)
```

---

## ğŸ¯ Flow 3: Manual Job Trigger (User click "Trigger" button)

**API Endpoint**: `POST /api/jobs/{id}/trigger`

**Handler**: `api/src/handlers/jobs.rs::trigger_job()`

```rust
pub async fn trigger_job(
    State(state): State<AppState>,
    Path(id): Path<Uuid>,
) -> Result<Json<SuccessResponse<Uuid>>, ErrorResponse>
```

**Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MANUAL TRIGGER FLOW                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Validate job exists
   â”œâ”€ JobRepository::find_by_id(id)
   â””â”€ If not found â†’ Return 404

2. Check concurrent execution
   â”œâ”€ If allow_concurrent = false:
   â”œâ”€ ExecutionRepository::has_running_execution(job_id)
   â””â”€ If has_running â†’ Return 500 "concurrent_execution_not_allowed"

3. Create execution record
   â”œâ”€ Generate execution_id
   â”œâ”€ Generate idempotency_key: "manual-{job_id}-{execution_id}"
   â”œâ”€ ExecutionRepository::create()
   â””â”€ INSERT INTO job_executions (
        id, job_id, idempotency_key, status='pending',
        trigger_source='manual', created_at
      )

4. Publish to NATS
   â”œâ”€ JobPublisher::publish(execution)
   â”œâ”€ Subject: jobs.job_stream.{job_id}
   â””â”€ Wait for ACK

5. Return execution_id to user
   â””â”€ Response: { "data": "execution-uuid" }

6. Worker picks up message (same as scheduled flow)
   â””â”€ See Phase 2 above
```

---

## ğŸ“Š Database Schema & State Transitions

### jobs table
```sql
CREATE TABLE jobs (
    id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL UNIQUE,
    description TEXT,
    enabled BOOLEAN DEFAULT true,
    timeout_seconds INTEGER DEFAULT 300,
    max_retries INTEGER DEFAULT 10,
    allow_concurrent BOOLEAN DEFAULT false,
    minio_definition_path VARCHAR(500) NOT NULL,
    trigger_config JSONB NOT NULL,  -- {"scheduled": bool, "manual": bool, "webhook": {...}}
    created_at TIMESTAMP NOT NULL,
    updated_at TIMESTAMP NOT NULL
);
```

### job_executions table
```sql
CREATE TABLE job_executions (
    id UUID PRIMARY KEY,
    job_id UUID NOT NULL REFERENCES jobs(id),
    idempotency_key VARCHAR(255) NOT NULL UNIQUE,
    status VARCHAR(50) NOT NULL,  -- pending, running, success, failed, timeout, dead_letter
    attempt INTEGER DEFAULT 1,
    trigger_source VARCHAR(50) NOT NULL,  -- scheduled, manual, webhook
    trigger_metadata JSONB,
    current_step VARCHAR(255),
    minio_context_path VARCHAR(500) NOT NULL,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    result TEXT,
    error TEXT,
    created_at TIMESTAMP NOT NULL
);

CREATE INDEX idx_job_executions_job_id ON job_executions(job_id);
CREATE INDEX idx_job_executions_status ON job_executions(status);
CREATE INDEX idx_job_executions_idempotency_key ON job_executions(idempotency_key);
```

### Execution Status Transitions
```
pending â†’ running â†’ success
                 â†’ failed
                 â†’ timeout
                 â†’ dead_letter (after max retries)
```

---

## ğŸ—„ï¸ MinIO Storage Structure

```
vietnam-cron/
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ {job_id}/
â”‚       â”œâ”€â”€ definition.json          # Job definition (steps, schedule, etc.)
â”‚       â””â”€â”€ executions/
â”‚           â””â”€â”€ {execution_id}/
â”‚               â”œâ”€â”€ context.json     # Job context (step outputs, variables)
â”‚               â””â”€â”€ files/           # Uploaded/processed files
â”‚                   â”œâ”€â”€ input/
â”‚                   â””â”€â”€ output/
```

### Job Definition JSON (definition.json)
```json
{
  "id": "job-uuid",
  "name": "Job Name",
  "description": "Description",
  "schedule": {
    "Cron": {
      "expression": "0 */10 * * * *",
      "timezone": "Asia/Ho_Chi_Minh"
    }
  },
  "steps": [
    {
      "id": "step-1",
      "name": "Fetch Data",
      "type": {
        "HttpRequest": {
          "url": "https://api.example.com/data",
          "method": "GET",
          "headers": {},
          "body": null,
          "auth": null
        }
      },
      "condition": null
    }
  ],
  "triggers": {
    "scheduled": true,
    "manual": true,
    "webhook": null
  },
  "enabled": true,
  "timeout_seconds": 300,
  "max_retries": 10,
  "allow_concurrent": false
}
```

### Job Context JSON (context.json)
```json
{
  "execution_id": "execution-uuid",
  "job_id": "job-uuid",
  "steps": {
    "step-1": {
      "status": "success",
      "data": "{\"temperature\": 25}",
      "metadata": {
        "status_code": 200,
        "duration_ms": 123
      },
      "completed_at": "2025-11-26T08:00:01Z"
    }
  },
  "variables": {
    "API_KEY": "secret-value"
  },
  "created_at": "2025-11-26T08:00:00Z",
  "updated_at": "2025-11-26T08:00:01Z"
}
```

---

## ğŸ”„ NATS JetStream Configuration

### Stream Configuration
```
Name: job_stream
Subjects: jobs.>
Retention: WorkQueue (messages deleted after ACK)
Max Age: 24 hours
Max Messages: 1,000,000
```

### Consumer Configuration
```
Name: worker-consumer
Durable: true
Ack Policy: Explicit (manual ACK required)
Max Deliver: 10 (max retry attempts)
Ack Wait: 5 minutes (timeout before redelivery)
```

### Message Flow
```
1. Scheduler publishes â†’ Stream stores message
2. Worker pulls message â†’ Stream marks as "delivered"
3. Worker processes job
4. Worker ACKs message â†’ Stream deletes message (WorkQueue)

If Worker NAKs or timeout:
â†’ Stream redelivers message (up to Max Deliver times)
â†’ After Max Deliver: Message goes to dead letter
```

---

## ğŸ” Redis Distributed Lock (RedLock)

**Purpose**: Ensure only one scheduler processes each job

**Flow**:
```
1. Scheduler tries to acquire lock
   â”œâ”€ SET NX EX schedule:job:{job_id} {random_value} 30
   â””â”€ If SET returns 1 â†’ Lock acquired

2. Scheduler processes job
   â””â”€ Create execution, publish to NATS

3. Scheduler releases lock
   â”œâ”€ Check lock value matches (prevent releasing other's lock)
   â””â”€ DEL schedule:job:{job_id}

If scheduler crashes:
â†’ Lock expires after 30 seconds (TTL)
â†’ Another scheduler can acquire lock
```

---

## ğŸ“ˆ Monitoring & Observability

### Logs (JSON format)
```json
{
  "timestamp": "2025-11-26T08:00:00Z",
  "level": "INFO",
  "message": "Processing job",
  "span": {
    "execution_id": "uuid",
    "job_id": "uuid",
    "name": "process_job"
  }
}
```

### Metrics (Prometheus)
```
# Job execution metrics
job_executions_total{status="success|failed|timeout"}
job_execution_duration_seconds{job_id="..."}
job_queue_depth{stream="job_stream"}

# System metrics
scheduler_poll_duration_seconds
worker_message_processing_duration_seconds
```

### Tracing (OpenTelemetry)
```
Trace: Job Execution
â”œâ”€ Span: scheduler.process_job
â”‚  â”œâ”€ Span: db.find_jobs_due
â”‚  â”œâ”€ Span: lock.acquire
â”‚  â””â”€ Span: queue.publish
â””â”€ Span: worker.process_job
   â”œâ”€ Span: db.load_job
   â”œâ”€ Span: minio.load_definition
   â”œâ”€ Span: executor.execute_step
   â””â”€ Span: minio.store_context
```

---

## ğŸ¯ Summary: Complete Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         JOB EXECUTION FLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME    SCHEDULER              POSTGRESQL         NATS          WORKER              MINIO
â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€
T+0s    Poll jobs due â”€â”€â”€â”€â”€â”€â”€â”€â–¶ SELECT jobs
        â”‚                       WHERE enabled=true
        â”‚                       AND scheduled=true
        â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚
T+1s    Check concurrent â”€â”€â”€â”€â”€â–¶ SELECT COUNT(*)
        execution               FROM job_executions
        â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚
T+2s    Acquire Redis lock
        (RedLock)
        â”‚
T+3s    Create execution â”€â”€â”€â”€â”€â–¶ INSERT INTO
        â”‚                       job_executions
        â”‚                       (status=pending)
        â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚
T+4s    Publish message â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Store in
        â”‚                                        job_stream
        â”‚                                        â”‚
        â”‚                                        â”‚
T+5s    Release lock                             â”‚
                                                 â”‚
                                                 â”‚
T+6s                                             â”‚ Pull msg â”€â”€â–¶ Receive
                                                 â”‚              message
                                                 â”‚              â”‚
T+7s                                             â”‚              Check â”€â”€â–¶ SELECT *
                                                 â”‚              idempo-  FROM
                                                 â”‚              tency    job_exec
                                                 â”‚              â”‚â—€â”€â”€â”€â”€â”€â”€â”€
                                                 â”‚              â”‚
T+8s                                             â”‚              Load â”€â”€â”€â–¶ SELECT *
                                                 â”‚              job      FROM jobs
                                                 â”‚              â”‚â—€â”€â”€â”€â”€â”€â”€â”€
                                                 â”‚              â”‚
T+9s                                             â”‚              Load â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ GET
                                                 â”‚              defini-              definition
                                                 â”‚              tion                 .json
                                                 â”‚              â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                                 â”‚              â”‚
T+10s                                            â”‚              Update â”€â–¶ UPDATE
                                                 â”‚              status   job_exec
                                                 â”‚              =running SET status
                                                 â”‚              â”‚â—€â”€â”€â”€â”€â”€â”€â”€
                                                 â”‚              â”‚
T+11s                                            â”‚              Execute
                                                 â”‚              Step 1
                                                 â”‚              (HTTP)
                                                 â”‚              â”‚
T+12s                                            â”‚              Store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ PUT
                                                 â”‚              context              context
                                                 â”‚              â”‚                    .json
                                                 â”‚              â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                                 â”‚              â”‚
T+13s                                            â”‚              Execute
                                                 â”‚              Step 2
                                                 â”‚              ...
                                                 â”‚              â”‚
T+14s                                            â”‚              Update â”€â–¶ UPDATE
                                                 â”‚              status   job_exec
                                                 â”‚              =success SET status
                                                 â”‚              â”‚â—€â”€â”€â”€â”€â”€â”€â”€
                                                 â”‚              â”‚
T+15s                                            â”‚              ACK msg
                                                 â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                                 â”‚
                                                 Delete msg
                                                 (WorkQueue)

```

---

**Táº¡o bá»Ÿi**: Kiro AI Assistant  
**NgÃ y**: 2025-11-26  
**Version**: 1.0
